---
kind: "article"
title: "Fin Data"
description: "Describes what Fin data is and how to format it for transmission or storage."
pubDate: "2024-01-01T23:38:33.561Z"
slug: "draft/fin-data"
---

> Unstable. Expect changes.

{% box %}

**Ideas**

- Standard set of **built-in tags** which libraries are expected to implement on best-effort basis?
  - local/bare symbols that end with `_` are reserved for this set.
  - can support different primitives depending on the target format.
    - cannot support custom binary layout must use primitives? is this true?
  - the set could grow over time?
  - `array_` and `map_` are implicit ?
  - `set_`, `ts_` (timestamp) remove timestamp literal?
  - perhaps with ubiquitous data types have syntax that is transformed into builtin tags.
    - could this work with maps?
- ~~Allow primitives with delimiters to be tagged (strings, byte arrays)~~
- `set_[1, 2, 3]`
- `url_["http://github.com"]`
- `uuid_["5f05a510-cde2-4f4e-80d7-6d94bc095688"]`
- `uuid_[<5f 05 a5 10 cd e2 4f 4e 80 d7 6d 94 bc 09 56 88>]`
- `ts_["2024-01-22T22:53:52.548Z"]`
- `rfc3339_["2024-01-22T22:53:52.548Z"]`
- `zdt_["2024-01-22T22:53:52.548Z", "Australia/Brisbane"]`

{% /box %}

This document describes what fin data is and how to format it for transmission or storage. This
includes a [data model](#fin-data-model) and two formats.

- A [text format](#fin-text-format) (`.fin` extension) for humans to read, author and manipulate
  with a text editor.
- A [binary format](#fin-binary-format) (`.fib` extension) for more efficient interchange between
  programs.

Both formats are **self-describing**, meaning aside from the rules outlined in this specification no
additional information or out of band schema is required to produce or interpret it.

## Motivating use cases

- Human authored records (configuration, metadata, documentation, logs etc.)
- Sending values efficiently between programs.
- For use in domains that require precise value types (e.g. financial applications).
- For archival and long term information preservation.
- To describe programs. It should be expressive/capable enough to use as the base syntax for general
  purpose and domain specific programming languages.

{% box %}

**Why another format?** For example, consider YAML which is common place. In fact, I can see some at
the top of this article in my text editor. Here is my favourite review of YAML

> nothing but a labyrinth of white space and inscrutable symbols, a prison for the unwary coder. It
> mocks my intelligence with its simplistic structure and minimal syntax, yet it still manages to
> ensnare me in its deadly embrace. I curse the day I first laid eyes on that ridiculous format, and
> the false promises of ease and clarity it offered me. It has only brought me pain and frustration,
> with its inconsistent indentation and subtle errors lurking in every line.
>
> [Andrew Kelley (creator of the Zig programming language)](https://github.com/ziglang/zig/pull/14265#issue-1528464069)

Also see
[The YAML document from hell](https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell).

JSON/TOML are certainly better, but also have downsides. Yet these formats are often
[chosen](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-formats.html), even
for DSLs. This [xkcd](https://xkcd.com/927/) comic is a classic, but there's no such thing as too
many ideas, especially if innovation is the goal. I'm frustrated with many existing data formats and
curious if I could do better.

{% /box %}

## Goals

In order of importance.

- **No versioning**. We'd like our formats to be enduring with stable semantics.
  - It's _okay_ to iterate and rapidly prototype prior to release (alpha).
  - If the released spec is broken, deficient or unusable that's also okay. We can go back to the
    drawing board and try again with a different name, a different spec.
  - Adding complexity to data format specs in anticipation of future versions is counterproductive.
    So is handling multiple versions in implementations.
- **Straightforward**. The syntax rules should be simple and obvious. In particular, working with
  fin data should quickly become automatic and take very little mental capacity.
  - In my personal experience JSON has this property. YAML does not.
  - There's nuance to this goal. Fewer syntax rules is usually a good thing but relying too much on
    a limited set of primitives **overloads** them which is another form of complexity.
- **Value semantics**. Fin formats aren't concerned with higher level concepts such as schemas,
  validation or remote procedure calls. There is no concept of objects or references. The formats
  are nothing more than a mechanism to combine and convey values.
- **Interoperability**. We'd like to allow a wide range of systems across diverse domains to convey
  most information seamlessly without resorting to custom application logic (extension).
  - The goal is to choose a **pragmatic set of primitives** that leverage existing standards for
    fundamental data types to establish common ground without introducing unacceptable complexity to
    syntax rules.
  - Certain domains require precise value types. It is preferable to have primitives that support
    this. Capable systems can interchange data seamlessly and less capable systems can decide how to
    act given their constraints.
  - Interoperability is desirable, however the design should not be driven by the choices made in
    existing programming languages or other formats. Encoders/Decoders should take a "best-effort"
    approach when mapping to a given language or non-fin format.
  - As an example, many languages have `null`, `nil`, `undefined` or `void` to represent the
    _absence_ of a value. Fin is about conveying values, so does not encode these types. Leaving
    them out of the data achieves the same result.
- **Extensible**. If the core (first-class) value types support extension, consumers and produces
  can compose them to signal domain-specific semantics and intermediaries can be oblivious to it.
  New specification versions are not required for every new use-case or custom data type.
- **Top Level Streaming**. The formats should support top-level value streams. This means a fin data
  payload can be conveyed as a sequence of values over a long-lived connection, and could grow to
  arbitrary length. We consider comprehensive streaming protocols to be a higher level concept (out
  of scope) that could be built on top of this specification.
- **Canonical output**. Text decoders are flexible to the layout of incoming fin data, encoders are
  not. There is one agreed upon way to output fin data and all encoders enforce it. Auto-formatting
  tools (e.g. prettier) are not required as a decode/encode roundtrip produces canonical output.
- **Compactness**. Where possible design choices should be made that lead to compact representation
  and efficient implementations but not at the cost of the goals listed above.
- **Tooling**. Developer experience is very much an auxiliary goal that only makes sense if primary
  goals are met.
  - Reference encoders/decoders and test cases.
  - A Tree-sitter grammar.
  - A language server for text editors.
  - A CLI for inspecting and converting between data formats.
- **Lossless conversion** between fin formats is not a goal.
  - For our benefit the text format provides multiple ways to represent the same value (e.g.
    [raw strings](#raw-strings) vs [escaped strings](#escaped-strings)). [Comments](#comments) are
    also provided for our benefit and do not represent values at all. To support these, text
    encoders and decoders have to keep track of extra metadata.
  - On the other hand the binary format is for efficient value exchange between programs and by
    design does not encode unnecessary metadata. Converting from the fin text format to binary and
    back is likely to produce different output that represents the same value.

## Fin Data Model

The fin data model describes a set of 8 value types.

- 6 primitive types ([byte arrays](#byte-arrays), [strings](#strings), [symbols](#symbols),
  [booleans](#booleans), [numbers](#numbers), [timestamps](#timestamps))
- 2 collection types ([arrays](#arrays), [maps](#maps))

{% box %}

**Fin data model vs JSON.** JSON has 6 value types:

- 4 primitive types (strings, numbers, booleans, null). In comparison, fin
  - also has strings, numbers and booleans.
  - does not have a null type.
  - introduces byte arrays, symbols and timestamps.
- 2 collection types (arrays, objects) are analogous to those in fin (arrays, maps).

{% /box %}

### Byte Arrays

A byte arrays represents a fixed-length array of bytes (octets).

- Byte arrays are used to directly embed raw binary data.
- No interpretation or processing occurs at all (opaque).
- Empty byte arrays (zero length) are allowed.

### Strings

Strings are used to convey Unicode text.

- The Unicode standard is ubiquitous and makes sense as a core primitive. In almost all cases, if a
  producer wants to convey text, it will be Unicode text. (interoperability)
- Encoders and decoders must ensure strings are a valid sequence of Unicode code points.
- No Unicode canonicalization occurs (out of scope).
- Empty strings are allowed.

### Symbols

Symbols are simplified strings used to represent names and identifiers.

- Restricted to the following code points (`a-z`, `0-9`, `:`, `_`)
  - Subset of the Basic Latin Unicode Block (one-byte in UTF-8).
- Empty (zero length) symbols are invalid.
- Must not start with a digit.
- `true` and `false` are invalid symbols
- The underscore is provided to break up words
  - Double underscore is not allowed.
  - Only `snake_case` is possible
- The colon is provided to support breaking up the symbol into segments.
  - Intended to allow higher level semantics like namespaces/aliasing.
  - Double colon is not allowed.
  - Symbols must not end with a colon.
- Symbols are part of fins extension mechanism (tagged collections).

{% box %}

**Why not use strings?** You could also ask why have strings when byte arrays work? Hopefully it's
obvious that strings are more useful to us than opaque blobs of hex or base64. In practice, the same
applies for symbols, they are more useful than strings in certain situations. You can make the case
that
[strings do too many things](https://buttondown.email/hillelwayne/archive/strings-do-too-many-things/)
and extracting symbols as a primitive reduces overloading.

One of the [motivating use cases](#motivating-use-cases) for fin data is to describe programs where
local names and identifiers are abundant. Having a restricted and more ergonomic primitive than
strings makes a big difference writing programs (see JSON DSLs). It's not just programming languages
though. If you look at most JSON payloads, object keys are almost always symbols and the use of
strings is unwarranted.

{% /box %}

### Booleans

Booleans have two possible values `true` and `false`.

### Numbers

Represent [real numbers](https://en.wikipedia.org/wiki/Real_number) using arbitrary precision
decimal notation.

- Includes all integers.
- In practice, only approximates some rationals.
- However can be combined (using tagged collections) to represent exact rationals and complex
  numbers.
- Does not represent special values (`nan`, `inf`, `-inf`) which are not real numbers.

### Timestamps

A timestamp is a date-time record that conveys an exact point in time.

- They have a consistent global definition.
  > A timestamp is a date-time clock measurement along with that clocks **offset** from UTC
  > ([Coordinated Universal Time](https://en.wikipedia.org/wiki/Coordinated_Universal_Timehttps://en.wikipedia.org/wiki/Coordinated_Universal_Time)).
- UTC is the primary time standard globally used to regulate clocks and time.
- A clock measurement includes a date component (conventionally using the Gregorian Calendar)
- Fractional seconds can be used to record time with arbitrary precision.
- It is possible for two different timestamps to represent the same instant in time (clocks with
  different offsets)
- Timestamps do not encode timezone or alternate calendar information. The expectation is authors
  will combine timestamps with other fin primitives for use cases that require it.

{% box %}

**Why are timestamps primitive?**

- Exact time is a fundamental part of information which is obvious to anyone keeping historical
  records.
- Its use is widespread in many different domains.
- A primitive reduces the scenario where authors are choosing different notation to represent the
  same instant in time. Interoperability improves.
- It's one less type to jam into strings which are already overloaded.

That being said, you could make the same argument for many data types (URLs, URIs, UUIDs, money,
emails, regex, etc.) to be blessed as primitives. It's a trade-off though, more primitives mean more
syntax rules. It becomes harder for us to parse data at a glance (especially when taken to the
extreme). There's also the awkward scenario when new standards emerge that _should_ be primitives
based on those included in the specification.

Ultimately we made the judgement call that exact time is fundamental and would benefit the most from
being primitive.

{% /box %}

### Arrays

Arrays are an **ordered** collection of values (elements).

- Each element can be of any type (heterogeneous) including collections making the definition of
  arrays recursive.
- Arrays can be tagged (prefixed) with a symbol. Consumers can interpret the tag as a signal that
  the array has extended semantics.
- Empty arrays (zero elements) are allowed.
- Duplicate elements are allowed. Use tagged arrays to extend arrays with set semantics.

### Maps

Maps are an **unordered** collection of pairs.

- A pair is a mapping (association) of a key to a value.
- Keys and values can be any type.
- Duplicate keys with different values are invalid (error).
  - Encoders _should_ not produce duplicate keys.
  - Decoders must ignore duplicate keys.
- Decoder implementations may expose the order of key-value pairs. Consumers can maintain this order
  but should not derive any meaning from it. The reason being intermediate processors may have
  legally changed the order from the original source. That's fine, maps convey a mapping of keys to
  values, independent of order.
- Like arrays Maps can be tagged with a symbol to signal extended semantics.
- Empty maps (zero pairs) are allowed.

{% box %}

**Why have a map type?** Technically arrays are enough for a working data model (see lisps and
s-expressions).

- An array of arrays (each length 2) could represent a map.
- However order is now significant, duplicate keys are not prohibited, and it's not obvious at a
  glance that the value is a mapping.
- Tagging the array can solve these issues, but now it's not automatic, all consumers must provide
  this logic and significantly more syntax is required.
- These downsides are magnified by how prolific "mapping data structures" are in computing and
  information exchange.
  - Look at almost any JSON payload in the wild, chances are it contains an object, often lots of
    them.

{% /box %}

### Top Level Semantics

A fin payload is always implicitly a map.

- This means fin data is created by encoding key-value pairs one at a time until there's no more
  input.
- The sequence of pairs is interpreted by the consumer as a map.
- The sequence may be transferred in its entirety or one pair at a time over a long-lived
  connection.

---

**WIP**

## Fin Text Format

- File extension is `.fin`
- MIME type is `application/fin`
- The fin text format is defined in terms of Unicode text (code points).
- Each code point is distinct and no Unicode normalisation is performed.
- The only supported character encoding is UTF-8.
- The UTF-8 byte order mark (BOM) may be stripped if present and disallowed from the remaining
  input.

**TODO** any illegal code points? NUL

### Normalisation

The fin text format is designed to be authored by humans and as such invalid syntax is to be
expected. In cases where the input is strictly invalid, but it can be determined precisely what the
user was intending (unambiguous fix) then decoders should accept the invalid code. They should
provide feedback to the user (via reporting) and fix it. This is a better experience than strictly
failing on invalid input.

**Normalisation only applies when decoding the text format**. This document will explicitly point
out when normalisation should happen. Compliant fin text decoders must provide two modes

- `strict`. Fail immediately when invalid input is found.
- `lax` (default). Collect issues, perform normalisation (as outlined in this document) and continue
  decoding where possible.

#### Global Rules

- Canonical output is free of carriage return (`"\r"`) code points. However, it can almost always
  safely be accepted and removed from canonical output
- Capital letters `A-Z` are invalid except as part of [comment](#comments) and [string](#strings)
  literals and can safely be normalised to lower case.

#### Syntax Forms

- Inline.
  - Starts and ends on the same line.
  - Optionally surrounded by delimiters.
- Block.
  - starts with a delimiter and extends to the end of line. If the first non whitespace code point
    on the next line is the block delimiter then the block expands to include that line, and so on.
- Expanded.
  - Contains sub-syntax that span multiple lines.
  - Optionally surrounded by delimiters.

### Comments

```
# this is a comment which extends to the next two lines.
# comments are verbatim utf-8 sequences with no escaping.
# if required, a space will be inserted after the initial #.

[
  # comments follow normal indentation rules.
  # this comment appears inside an array.
  # as a result the arrays canonical output has expanded.
  1, 2
  3
]

## comments that start with double-hash are auto wrapped at 65
## character width (not including `## `).
```

- Exist only for human communication / documentation. This means they're only found in the text
  format.
- They do not represent values. The array in the above example contains three numbers, the comment
  does not count as a value.
- By definition comments extend at least to the end of the line.
  - It's impossible to output an aggregate map or array on a single line if it contains a comment.
  - This property means authors can use comments to control the canonical text output of aggregates.
- Trailing comments are not allowed and will be moved to a new line.
- Syntax classification: `non-value`, `block`.

### Gaps

**TODO**

- vertical space
- sub-syntax element seen in expanded aggregates.
- Syntax classification: `non-value`, `block`.

### Booleans

```
true
false
```

- Uppercase will be normalised to lowercase.
- Syntax classification: `primitive`, `inline`.

### Symbols

```
abc
_
:println
_abc:d12:e_3
:crypto:sha256
```

- A string-like primitive with no wrapping delimiters.
  - They are a natural fit to represent names and identifiers where a string would be more
    cumbersome.
- `camelCase` and `kebab-case` are accepted but normalised to snake case. Any remaining uppercase
  code points are converted to lowercase.
- Syntax classification: `primitive`, `inline`.

### Strings

String literals are used to convey Unicode text. Text encoders and decoders ensure strings are a
valid sequence of Unicode code points. Two syntax forms are provided for strings.

#### Escaped Strings

```
"this \t string \n supports \\ escape \" sequences \u{1f602}"
"" # empty strings are okay
```

- Newline, backslash and double quote are not allowed unescaped.
- Common escapes. `\n \r \t \" \\`
- Unicode escapes. Code point is specified with hex digits `'\u{1f602}'`.
  - All uppercase is normalised to lowercase.
  - Leading zeros are removed.
- No byte escapes? Are they okay if encoders/decoders enforce valid Unicode?
- Syntax classification: `primitive`, `inline`.

#### Raw Strings

```
|raw string | no escapes so newlines
|and comments are # part of the raw string

[
  # the two strings below are the same
  | another raw string this time indented
  |  #\n|#
  " another raw string this time indented\n  #\\n|#"
]

# below is an empty raw strings
|
```

- Very similar to [comments](#comments) but are values
- also force aggregates to expand
- **TODO** can they be trailing? Does line normalisation still apply?
- Syntax classification: `primitive`, `block`.

{% box %}

**why no inline raw strings?**

- Allow inline raw strings?
  - `'a raw \n string'`
    - but these would have to escape at least `\n`
    - or make it illegal, or change into expanded form by formatter...
    - if raw strings contain `\n` they use `|` otherwise they use `''`
    - `'` would need to be escaped though?
    - alternatively use backticks and just make internal backticks illegal like golang
  - `"esc"` is sugar for `str:'esc'` ?

{% /box %}

### Bytes

```
<00>, <de ad be ef> # spaces between bytes/octets?
<> # zero bytes is ok
```

- raw data (byte arrays, bytes) are represented inline in hexadecimal notation between angle
  brackets
- uppercase will be normalised to lowercase.
- Syntax classification: `primitive`, `inline`.

### Numbers

```
0, -1, 42
0.0, -.123, 1.23e+10
```

TODO combine integer?

- Arbitrary precision integers are written verbatim in decimal notation.
- Common prefixes for binary `0b`, octal `0o` and hexadecimal `0x` are accepted but will normalise
  to decimal notation. See [bytes](#bytes) for a hex literal. **TODO** is this the best choice?
- All leading `+` signs will be stripped.
- A single `-` sign is considered part of the integer. It will be stripped from zero.
- Internal underscores `_` are often used to visually separate number digits. They are accepted but
  will be stripped when normalising.
- Syntax classification: `primitive`, `inline`.

- Arbitrary precision decimals
- Similar to integers underscores `_` will be stripped.
- alternate base notation will be converted to decimal.
- Syntax classification: `primitive`, `inline`.

### Timestamps

```
2024-01-22T22:53:52.548Z
2007-02-23T12:14:33.079-08:00
```

- strict form of RFC3339. normalisations?
- Syntax classification: `primitive`, `inline`.

### Arrays

```

# inline
[1, 2, true]

[
  # expanded
  42
]

```

- Syntax classification: `aggregate`, `inline` & `expanded`.

### Maps

```
# call to array of key-value pairs. 1 symbol, 6 delimiters, 3 commas
map[[k1, v1], [k2, v2]]

# same but using map syntax. 2 delimiters, 1 comma
(k1 v1, k2 v2)
```

- Technically an aggregate for a sequence of items (e.g. [arrays](#arrays)) is enough for a working
  data format. For example consider lisp and s-expressions.
- Why have different syntax for maps?
  - Maps are everywhere. Most JSON payloads have more objects than arrays.
  - If built on top of arrays. They require significantly more delimiters and commas.
  - A single starting symbol is required to signal to the reader that it's a map which may not
    always be visible.
  - A different syntax is less verbose and makes it immediately obvious it's a map.
- Syntax classification: `aggregate`, `inline` & `expanded`.

#### Top-level Map

```
a 1, b 2
c 3

letters ["a", "b", "c"]
numbers [1, 2, 3]
```

- The top level value is always an implicit, expanded map. Implicit means the opening/closing map
  delimiters `()` are implied when processing the top level (root) value.
- Key value pairs are listed with no indentation.
- Otherwise all the same syntax rules of an expanded map apply.
- Syntax classification: `aggregate`, `expanded`.

### Calls

**TODO** remove

```
hello["world"]
run(verbose true)
foo[bar[baz[]]
foo[
  bar[
    # this comment expands the bar call
    # which in turn expands the foo call
    # the baz call remains inline
    baz[]
  ]
]

# extension could even be used to rewrite nested calls (multiple tags)
thread_first[[], baz, bar, foo]

# more powerful than a simple list of tags
thread_first[42, inc, add[2, 3]]
add[inc[42], 2, 3]

thread_last[42, inc, add[2, 3]]
add[2, 3, inc[42]]

thread_as[(x 42), foo(a x), add[2, x, 3]]
add[2, foo[a 42], 3]
```

- Unlike array/maps which can contain any number of sub-syntax elements calls always contain exactly
  two sub elements, and they're types are restricted.
  - The first element (the tag) must be a [symbol](#symbols)
  - The second element (the argument) must be an [array](#arrays) or a [map](#maps).
  - The above restrictions allow call syntax to be defined without delimiters.
  - Any symbol that immediately precedes an array or map is a valid call.
  - Calls can be thought of as tagged values.
- While array syntax could be overloaded to provide an extension mechanism (e.g. s-expressions)
  calls provide a simple, obvious extension point for authors.
- Why only a single tag per call?
  - Some formats provide the ability to use multiple tags/annotations? They usually require extra
    syntax delimiters.
  - Extension even in the single-tag form described here is surprisingly powerful.
  - See the `thread_` examples above for how you could build multi-tag semantics using only
    single-tag calls.
- Syntax classification: `aggregate`, `inline` & `expanded`.
  - The call inherits the same form (`inline`/`expanded`) as its argument

## Fin Binary Format

- File extension is `.fib`
- MIME type is `application/fib`?
