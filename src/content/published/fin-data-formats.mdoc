---
kind: "article"
title: "Fin Data Formats"
description: "Describes what Fin data is and how to format it for transmission or storage"
pubDate: "2024-01-01T23:38:33.561Z"
slug: "draft/fin-data-formats"
---

> This is a draft / alpha

This document describes what Fin data is and how to format it for transmission or storage. Two
formats are provided.

- A [text format](#fin-text-format) (`.fin` extension), designed primarily for humans to read,
  author and manipulate with a text editor.
- A [binary format](#fin-binary-format) (`.fib` extension), designed to efficiently convey data
  between programs.

Both formats are **self-describing**, meaning aside from this specification no additional
information or out of band schema is required to interpret it.

## Motivating use cases

- Human authored records. Configuration, documentation, change logs etc.
- Sending values efficiently between programs.
- Precise value types for domains that require it (financial applications).
- Long term data preservation.
- To describe programs. It should be expressive/capable enough to use as the base syntax for general
  purpose and domain specific programming languages.

{% box %}

**Why another format?** YAML is _still_ common place. In fact, I can see some at the top of this
article in my text editor. Here is my favorite review of YAML

> nothing but a labyrinth of white space and inscrutable symbols, a prison for the unwary coder. It
> mocks my intelligence with its simplistic structure and minimal syntax, yet it still manages to
> ensnare me in its deadly embrace. I curse the day I first laid eyes on that ridiculous format, and
> the false promises of ease and clarity it offered me. It has only brought me pain and frustration,
> with its inconsistent indentation and subtle errors lurking in every line.
>
> [Andrew Kelley (creator of the Zig programming language)](https://github.com/ziglang/zig/pull/14265#issue-1528464069)

JSON/TOML are certainly better, but also have downsides. Yet these formats are often
[chosen](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-formats.html), even
for DSLs. This [xkcd](https://xkcd.com/927/) comic is a classic, but there's no such thing as too
many ideas, especially if innovation is the goal. I'm frustrated with many existing data formats and
curious if I could do better.

{% /box %}

## Goals

- **Straightforward**. The syntax rules should be simple and obvious. Working with fin data should
  quickly become automatic and take very little mental capacity. In my personal experience JSON has
  this property. YAML does not.
  - There's nuance to this goal. Less syntax is often better, but care must be taken to **avoid
    overloading** which increases the mental capacity required by us to process the text format.
- **Value semantics**. Fin formats aren't concerned with higher level concepts such as schemas,
  validation or remote procedure calls. There is no concept of objects or reference types. The
  formats are nothing more than a mechanism to convey values.
- **Canonical output**. Text decoders are flexible to the layout of incoming fin data, encoders are
  not. There is one agreed upon way to output fin data and all encoders enforce it. Auto-formatting
  tools (e.g. prettier) are not required as a decode/encode roundtrip produces canonical output.
- **No versioning**. We'd like our formats to be enduring with stable semantics.
  - It's okay to iterate and rapidly prototype prior to release (alpha).
  - If the released spec is broken, deficient or unusable that's okay. We can make a new one with a
    different name.
  - Adding complexity to simple data formats in anticipation of future requirements/versions is
    counterproductive.
- **Extensible**. If the core (first-class) data types are chosen correctly, consumers and produces
  can compose them to convey domain-specific semantics and intermediaries can be oblivious to it.
  New specification versions are not required for every new use-case or custom data type. They can
  be built on top the core types.
- Tools for working with fin data.
  - Reference encoders/decoders and test cases.
  - A CLI for inspecting and converting between data formats.
  - A language server for text editors.

## Non-Goals / Out of scope

- **Compactness**. This is not to say that the formats presented here are not compact or efficient.
  Just that other goals were not compromised to achieve more compact representations.
- **Language mappings**. How these formats map to the data types provided by different languages and
  runtimes is not addressed.
- **Streaming**. Streaming applications have a different set of constraints and considerations but
  can be thought of as a sequence of small values (chunks). In this framing the formats presented
  here may or may not be enough to build streaming encoders and decoders. Perhaps an additional
  specification or format is required. In any case streaming and the constraints that go with it are
  out of scope and not considered for this specification.
- **Lossless conversion between formats**. The binary format is designed for value exchange between
  programs and as such does not encode comments which exist for human purposes (documentation and
  controlling layout). Converting text to binary will strip comments but maintain value equivalence
  (which is a goal).

# Fin Text Format

- File extension is `.fin`
- MIME type is `application/fin`
- The fin text format is defined in terms of Unicode text (code points).
- Each code point is distinct and no Unicode normalization is performed.
- The only supported character encoding is UTF-8.
- The UTF-8 byte order mark (BOM) may be stripped if present and disallowed from the remaining
  input.

**TODO** TOC links

**TODO** any illegal code points? NUL

## Normalization

The fin text format is designed to be authored by humans and as such invalid syntax is to be
expected. In cases where the input is strictly invalid, but it can be determined precisely what the
user was intending (unambiguous fix) then decoders should accept the invalid code. They should
provide feedback to the user (via reporting) and fix it. This is a better experience than strictly
failing on invalid input.

**Normalization only applies when decoding the text format**. This document will explicitly point
out when normalization should happen. Compliant fin text decoders must provide two modes

- `strict`. Fail immediately when invalid input is found.
- `lax` (default). Collect issues, perform normalization (as outlined in this document) and continue
  decoding where possible.

**TODO** Line normalization. any other global normalization?

## Comments

```
# this is a comment which extends to the next two lines.
# comments are verbatim utf-8 sequences with no escaping.
# if required, a space will be inserted after the initial #.

[1, 2, 3] # this is a trailing comment which terminates at EOL.
# this is a separate comment from the one above.

[
  # comments follow normal indentation rules.
  # this comment appears inside an array.
  # as a result the arrays canonical output has expanded.
  1, 2
  3
]

## comments that start with double-hash are auto wrapped at 65
## character width (not including `## `).
```

- Exist only for human communication / documentation. This means they're only found in the text
  format.
- They do not represent values. The array in the above example contains three numbers, the comment
  does not count as a value.
- By definition comments extend at least to the end of the line.
  - It's impossible to output an aggregate map or array on a single line if it contains a comment.
  - This property means authors can use comments to control the canonical text output of aggregates.

[agg]: #aggregate-value-types
[prim]: #primitive-value-types

## Primitive value types

### Boolean

```
true
false
```

- Uppercase will be normalized to lowercase.

### Symbols

```
abc
_
:println
_abc:d12:e_3
:crypto:sha256
```

- A string-like primitive with no wrapping delimiters.
  - They are a natural fit to represent names and identifiers where a string would be more
    cumbersome.
- One of the stated [use cases](#motivating-use-cases) for fin data is to describe programs where
  local names and identifiers are abundant.
  - Having a more ergonomic syntax than strings provides a huge benefit to authors and consumers.
  - It's not just programming languages though. If you look at most JSON payloads, object keys are
    almost always symbols and the use of strings is unnecessary.
- Symbols are an essential part of fins own [extension syntax](#calls)
- They are restricted to the following subset of code points (`a-z`, `0-9`, `:`, `_`)
  - Must not start with a digit.
  - The underscore is provided to break up words.
    - Only `snake_case` is possible.
    - `camelCase` and `kebab-case` are accepted but normalized to snake case. Any remaining
      uppercase code points are converted to lowercase.
  - The colon is provided to support breaking up the symbol into segments. This allows symbols to be
    used for higher level semantics like namespaces/aliasing without resorting to strings.
  - Symbol code points are a subset of the Basic Latin Unicode Block and by definition are all
    one-byte in UTF-8. They're more compact and simpler to decode than strings.

---

**WIP**

### Strings

String literals are used to convey Unicode text.

- They are guaranteed to represent a valid sequence of Unicode code points.

  - Encoders/Decoders enforce this.

- A string literal that supports escape sequences.
- Newline, backslash double quote are not allowed unescaped.
- Common escapes. `\n \r \t \" \\`
- Unicode escapes. `\u{1f602}` Same as code points

```

"this \t string \n supports \\ escape \" sequences \u{1f602}"

```

**TODO** absorb into string. Code point literals are delimited by **single** quotes and are used to
represent a **single** Unicode code point. Ultimately they are integers (in the range 0-0x10FFFF)
which defines their position in the Unicode code space.

- There are two types of code point literals
  - Unescaped (single character).
    - A valid UTF-8 byte sequence between single quotes. For example `'a'` is code point `97`.
    - Newline and single quote are not allowed unescaped.
    - More than one code point is a syntax error `'ab'`.
  - Escaped sequences (backslash followed by one or more characters).
    - Common.`'\n', '\r', '\t', '\''`
    - Unicode. Codepoint is specified with hex digits `'\u{1f602}'`.
      - All uppercase is normalized to lowercase.
      - Leading zeros are removed.
- **TODO** Should surrogate code points be dis-allowed? Any others?
- There are alternate ways to represent code points.
  - One option is to not use them at all and overload strings instead (e.g. javascript)
    - This actually doesn't seem too bad.
  - Another is to use [extension](#calls) `code_point[97]`
- The escape syntax in particular code be more compact however it was chosen to match the escapes
  used in [string literals](#strings)

### Raw Strings

```
|raw string | no escapes
|comments are # part of the string
```

- Block syntax is similar to comments accept they are treated as values
- also force aggregates to expand
- **TODO** can they be trailing?

### Bytes

- raw data (byte arrays, bytes) are represented inline in hexadecimal notation between angle
  brackets
- uppercase will be normalized to lowercase.

```

<00>, <de ad be ef> # spaces between bytes/octets

```

### Integers

- Arbitrary precision integers are written verbatim in decimal notation.
- Common prefixes for binary `0b`, octal `0o` and hexadecimal `0x` are accepted but will normalize
  to decimal notation. See [bytes](#bytes) for a hex literal.
- All leading `+` signs will be stripped.
- A single `-` sign is considered part of the integer. It will be stripped from zero.
- Internal underscores `_` are often used to visually separate number digits. They are accepted but
  will be stripped when normalizing.

```

0, -1, 42

```

### Decimals

- Arbitrary precision decimals
- Similar to integers underscores `_` will be stripped.
- alternate base notation will be converted to decimal.

```

0.0, -.123, 1.23e+10 nan, inf, -inf # ?

```

### Rationals

TODO

- why?
- can an extension be used?
- what about complex numbers?

```

1/2, -12/3 1/0, -1/0, 0/0

```

### Instants

TODO.

- why?
- can an extension be used?

## Aggregate value types

### Arrays

```

[1, 2, true] # inline

[

# expanded

42 ]

```

### Maps

- Technically an aggregate for a sequence of items (e.g. [arrays](#arrays)) is enough for a working
  data format. For example consider lisp and s-expressions.
- Why have different syntax for maps?
  - Maps are everywhere. Most JSON payloads have more objects than arrays.
  - If built on top of arrays. They require significantly more delimiters and commas.
  - A single starting symbol is required to signal to the reader that it's a map which may not
    always be visible.
  - A different syntax is less verbose and makes it immediately obvious it's a map.
- Other than the syntax change semantically it's equiv to an array of key value pairs
  - Two maps with different order are two different values. Is this ok?
  - Consumers can decide if order is significant or not

```

map[[k1, v1], [k2, v2]] # call to array of key-value pairs. 1 symbol, 6 delimiters, 3 commas

(k1 v1, k2 v2) # same but using map syntax. 2 delimiters, 1 comma

map[

# expanded. 1 symbol, 6 delimiters, 2 commas

[k1, v1] [k2, v2] ] (

# expanded. 2 delimiters

a 1 b 2 )

# it's clear that the map syntax is a lot more minimal

```

### Calls

TODO.

- why?
- why only single tag?

```

print["hello", "world"]

```

## Top-level value

TODO. Implicit map.

# Fin Binary Format

Extension `.fib`

> In Development

## Comments

Comments only exists in the text format. Binary encoders should provide an API that accepts comments
and simply drops them.
