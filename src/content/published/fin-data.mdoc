---
kind: "article"
title: "Fin Data"
description: "Describes what Fin data is and how to format it for transmission or storage."
pubDate: "2024-01-01T23:38:33.561Z"
slug: "draft/fin-data"
---

> Alpha. Expect changes.

This document describes what Fin data is and how to format it for transmission or storage. This
includes a [data model](#fin-data-model) and two formats.

- A [text format](#fin-text-format) (`.fin` extension) for humans to read, author and manipulate
  with a text editor.
- A [binary format](#fin-binary-format) (`.fib` extension) to more efficiently convey data between
  programs.

Both formats are **self-describing**, meaning aside from this specification no additional
information or out of band schema is required to produce or interpret it.

## Motivating use cases

- Human authored records. Configuration, metadata, documentation, logs etc.
- Sending values efficiently between programs.
- For use in domains that require precise value types (e.g. financial applications).
- For archival and long term information preservation.
- To describe programs. It should be expressive/capable enough to use as the base syntax for general
  purpose and domain specific programming languages.

{% box %}

**Why another format?** For example, consider YAML which is common place. In fact, I can see some at
the top of this article in my text editor. Here is my favourite review of YAML

> nothing but a labyrinth of white space and inscrutable symbols, a prison for the unwary coder. It
> mocks my intelligence with its simplistic structure and minimal syntax, yet it still manages to
> ensnare me in its deadly embrace. I curse the day I first laid eyes on that ridiculous format, and
> the false promises of ease and clarity it offered me. It has only brought me pain and frustration,
> with its inconsistent indentation and subtle errors lurking in every line.
>
> [Andrew Kelley (creator of the Zig programming language)](https://github.com/ziglang/zig/pull/14265#issue-1528464069)

Also see
[The YAML document from hell](https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell).

JSON/TOML are certainly better, but also have downsides. Yet these formats are often
[chosen](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-formats.html), even
for DSLs. This [xkcd](https://xkcd.com/927/) comic is a classic, but there's no such thing as too
many ideas, especially if innovation is the goal. I'm frustrated with many existing data formats and
curious if I could do better.

{% /box %}

## Goals

- **Straightforward**. The syntax rules should be simple and obvious. In particular, working with
  fin data should quickly become automatic and take very little mental capacity. In my personal
  experience JSON has this property. YAML does not.
  - There's nuance to this goal. Less syntax is often better, but care must be taken to **avoid
    overloading** which increases the mental capacity required by us to process syntax.
- **Value semantics**. Fin formats aren't concerned with higher level concepts such as schemas,
  validation or remote procedure calls. There is no concept of objects or reference types. The
  formats are nothing more than a mechanism to convey values.
- **Canonical output**. Text decoders are flexible to the layout of incoming fin data, encoders are
  not. There is one agreed upon way to output fin data and all encoders enforce it. Auto-formatting
  tools (e.g. prettier) are not required as a decode/encode roundtrip produces canonical output.
- **No versioning**. We'd like our formats to be enduring with stable semantics.
  - It's _okay_ to iterate and rapidly prototype prior to release (alpha).
  - If the released spec is broken, deficient or unusable that's also okay. We can go back to the
    drawing board and try again with a different name.
  - Adding complexity to simple data formats in anticipation of future requirements/versions is
    counterproductive.
- **Extensible**. If the core (first-class) data types are chosen correctly, consumers and produces
  can compose them to convey domain-specific semantics and intermediaries can be oblivious to it.
  New specification versions are not required for every new use-case or custom data type. They can
  be built on top the core types.
- **Pragmatic primitives**. A set of primitive data types that support a wide range of domains and
  [use cases](#motivating-use-cases), leverage existing standards and increase interoperability
  without introducing unacceptable complexity to syntax rules and implementations.
- **Tooling**. Developer experience is very much an auxiliary goal that only makes sense if primary
  goals are met.
  - Reference encoders/decoders and test cases.
  - A Tree-sitter grammar.
  - A language server for text editors.
  - A CLI for inspecting and converting between data formats.

## Non-Goals / Out of scope

- **Compactness**. This is not to say that the formats presented here are not compact or efficient.
  Just that other goals were not compromised to achieve more compact representations.
- **Language mappings**. How these formats map to the data types provided by different languages and
  runtimes is not addressed.
  - In particular, many languages have `null`, `nil`, `undefined` or `void` types to represent the
    _absence_ of a value. Fin is about conveying values, so does not encode these types. Leaving
    them out of the data achieves the same result.
- **Streaming**. Streaming applications have a different set of constraints and considerations but
  can be thought of as a sequence of small values (chunks). In this framing the formats presented
  here may or may not be enough to build streaming encoders and decoders. Perhaps an additional
  specification or format is required. In any case streaming and the constraints that go with it are
  out of scope and not considered for this specification.
- **Lossless conversion between formats**. For our benefit the text format provides multiple ways to
  represent the same value (e.g. [raw strings](#raw-strings) vs
  [escaped strings](#escaped-strings)). [Comments](#comments) are also provided for our benefit and
  do not represent values at all. To support these, text encoders and decoders have to keep track of
  extra metadata.

  On the other hand the binary format is for efficient value exchange between programs and by design
  does not encode unnecessary metadata. Converting from the fin text format to binary and back is
  likely to produce different output that represents the same value.

## Fin Data Model

**TODO**

## Fin Text Format

- File extension is `.fin`
- MIME type is `application/fin`
- The fin text format is defined in terms of Unicode text (code points).
- Each code point is distinct and no Unicode normalisation is performed.
- The only supported character encoding is UTF-8.
- The UTF-8 byte order mark (BOM) may be stripped if present and disallowed from the remaining
  input.

**TODO** any illegal code points? NUL

### Normalisation

The fin text format is designed to be authored by humans and as such invalid syntax is to be
expected. In cases where the input is strictly invalid, but it can be determined precisely what the
user was intending (unambiguous fix) then decoders should accept the invalid code. They should
provide feedback to the user (via reporting) and fix it. This is a better experience than strictly
failing on invalid input.

**Normalisation only applies when decoding the text format**. This document will explicitly point
out when normalisation should happen. Compliant fin text decoders must provide two modes

- `strict`. Fail immediately when invalid input is found.
- `lax` (default). Collect issues, perform normalisation (as outlined in this document) and continue
  decoding where possible.

#### Global Rules

- Canonical output is free of carriage return (`"\r"`) code points. However, it can almost always
  safely be accepted and removed from canonical output
- Capital letters `A-Z` are invalid except as part of [comment](#comments) and [string](#strings)
  literals and can safely be normalised to lower case.

#### Syntax Forms

- Inline.
  - Starts and ends on the same line.
  - Optionally surrounded by delimiters.
  - [booleans](#booleans), [symbols](#symbols), [strings](#strings), [bytes](#bytes),
    [integers](#integers), [decimals](#decimals), [rationals](#rationals), [instants](#instants),
- Block.
  - starts with a delimiter and extends to the end of line. If the first non whitespace code point
    on the next line is the block delimiter then the block expands to include that line, and so on.
  - [comments](#comments), [gaps](#gaps)
- Expanded.
  - Contains sub-syntax that span multiple lines.
  - Optionally surrounded by delimiters.

#### Syntax Category

- non-value (2).
  - [comments](#comments), [gaps](#gaps)
- primitives (8).
  - [booleans](#booleans), [symbols](#symbols), [strings](#strings), [bytes](#bytes),
    [integers](#integers), [decimals](#decimals), [rationals](#rationals), [instants](#instants)
- aggregates (3).
  - [arrays](#arrays), [maps](#maps), [calls](#calls)
  - combine other syntax elements.
  - sub-syntax elements can also be aggregates (recursive nesting).

### Comments

```
# this is a comment which extends to the next two lines.
# comments are verbatim utf-8 sequences with no escaping.
# if required, a space will be inserted after the initial #.

[
  # comments follow normal indentation rules.
  # this comment appears inside an array.
  # as a result the arrays canonical output has expanded.
  1, 2
  3
]

## comments that start with double-hash are auto wrapped at 65
## character width (not including `## `).
```

- Exist only for human communication / documentation. This means they're only found in the text
  format.
- They do not represent values. The array in the above example contains three numbers, the comment
  does not count as a value.
- By definition comments extend at least to the end of the line.
  - It's impossible to output an aggregate map or array on a single line if it contains a comment.
  - This property means authors can use comments to control the canonical text output of aggregates.
- Trailing comments are not allowed and will be moved to a new line.
- Syntax classification: `non-value`, `block`.

### Gaps

**TODO**

- vertical space
- sub-syntax element seen in expanded aggregates.
- Syntax classification: `non-value`, `block`.

### Booleans

```
true
false
```

- Uppercase will be normalised to lowercase.
- Syntax classification: `primitive`, `inline`.

### Symbols

```
abc
_
:println
_abc:d12:e_3
:crypto:sha256
```

- A string-like primitive with no wrapping delimiters.
  - They are a natural fit to represent names and identifiers where a string would be more
    cumbersome.
- One of the stated [use cases](#motivating-use-cases) for fin data is to describe programs where
  local names and identifiers are abundant.
  - Having a more ergonomic syntax than strings provides a huge benefit to authors and consumers.
  - It's not just programming languages though. If you look at most JSON payloads, object keys are
    almost always symbols and the use of strings is unnecessary.
- Symbols are an essential part of fins own [extension](#calls) mechanism.
- They are restricted to the following subset of code points (`a-z`, `0-9`, `:`, `_`)
  - Must not start with a digit.
  - The underscore is provided to break up words.
    - Only `snake_case` is possible.
    - `camelCase` and `kebab-case` are accepted but normalised to snake case. Any remaining
      uppercase code points are converted to lowercase.
  - The colon is provided to support breaking up the symbol into segments. This allows symbols to be
    used for higher level semantics like namespaces/aliasing without resorting to strings.
  - Symbol code points are a subset of the Basic Latin Unicode Block and by definition are all
    one-byte in UTF-8. They're more compact and simpler to decode than strings.
- Syntax classification: `primitive`, `inline`.

---

**WIP**

### Strings

String literals are used to convey Unicode text. Text encoders and decoders ensure strings are a
valid sequence of Unicode code points. Two syntax forms are provided for strings.

#### Escaped Strings

```
"this \t string \n supports \\ escape \" sequences \u{1f602}"
"" # empty strings are okay
```

- Newline, backslash and double quote are not allowed unescaped.
- Common escapes. `\n \r \t \" \\`
- Unicode escapes. Code point is specified with hex digits `'\u{1f602}'`.
  - All uppercase is normalised to lowercase.
  - Leading zeros are removed.
- No byte escapes? Are they okay if encoders/decoders enforce valid Unicode?
- Syntax classification: `primitive`, `inline`.

#### Raw Strings

```
|raw string | no escapes so newlines
|and comments are # part of the raw string

[
  # the two strings below are the same
  | another raw string this time indented
  |  #\n|#
  " another raw string this time indented\n  #\\n|#"
]

# below is an empty raw strings
|
```

- Very similar to [comments](#comments) but are values
- also force aggregates to expand
- **TODO** can they be trailing? Does line normalisation still apply?
- Syntax classification: `primitive`, `block`.

### Bytes

```
<00>, <de ad be ef> # spaces between bytes/octets?
<> # zero bytes is ok
```

- raw data (byte arrays, bytes) are represented inline in hexadecimal notation between angle
  brackets
- uppercase will be normalised to lowercase.
- Syntax classification: `primitive`, `inline`.

### Numbers

#### Integers

```
0, -1, 42
```

- Arbitrary precision integers are written verbatim in decimal notation.
- Common prefixes for binary `0b`, octal `0o` and hexadecimal `0x` are accepted but will normalise
  to decimal notation. See [bytes](#bytes) for a hex literal. **TODO** is this the best choice?
- All leading `+` signs will be stripped.
- A single `-` sign is considered part of the integer. It will be stripped from zero.
- Internal underscores `_` are often used to visually separate number digits. They are accepted but
  will be stripped when normalising.
- Syntax classification: `primitive`, `inline`.

#### Decimals

```
0.0, -.123, 1.23e+10 nan, inf, -inf
```

- Arbitrary precision decimals
- Similar to integers underscores `_` will be stripped.
- alternate base notation will be converted to decimal.
- Syntax classification: `primitive`, `inline`.

#### Rationals

```
1/2, -12/3, 1/0, -1/0, 0/0
```

- **TODO** why?
- can an extension be used?
- what about complex numbers?
- Syntax classification: `primitive`, `inline`.

### Instants

```
2024-01-22T22:53:52.548Z
2007-02-23T12:14:33.079-08:00
```

- A primitive to represent an instant in time.
  - Also called an **exact time** which is the same everywhere. It has a basis, a consistent global
    definition.
  - Is unlike **wall-clock time** or **local time** which requires additional information to convey
    something meaningful.
- Why not use an extension?

  - exact time is a fundamental part of information which is obvious when keeping historical
    records. Its use is widespread.
  - We could use an extension over raw bytes to represent strings or integers for example, but we
    can all agree having string and number literals is advantageuos.
  - Without a literal many different formats are used and we lose interop, basis, connection.

- time or date only?
- Syntax classification: `primitive`, `inline`.

### Arrays

```

# inline
[1, 2, true]

[
  # expanded
  42
]

```

- Syntax classification: `aggregate`, `inline` & `expanded`.

### Maps

```
# call to array of key-value pairs. 1 symbol, 6 delimiters, 3 commas
map[[k1, v1], [k2, v2]]

# same but using map syntax. 2 delimiters, 1 comma
(k1 v1, k2 v2)
```

- Technically an aggregate for a sequence of items (e.g. [arrays](#arrays)) is enough for a working
  data format. For example consider lisp and s-expressions.
- Why have different syntax for maps?
  - Maps are everywhere. Most JSON payloads have more objects than arrays.
  - If built on top of arrays. They require significantly more delimiters and commas.
  - A single starting symbol is required to signal to the reader that it's a map which may not
    always be visible.
  - A different syntax is less verbose and makes it immediately obvious it's a map.
- Other than the syntax change semantically it's equiv to an array of key value pairs
  - Two maps with different order are two different values. Is this ok?
  - Consumers can decide if order is significant or not
- Syntax classification: `aggregate`, `inline` & `expanded`.

#### Top-level Map

```
a 1, b 2
c 3

letters ["a", "b", "c"]
numbers [1, 2, 3]
```

- The top level value is always an implicit, expanded map. Implicit means the opening/closing map
  delimiters `()` are implied when processing the top level (root) value.
- Key value pairs are listed with no indentation.
- Otherwise all the same syntax rules of an expanded map apply.
- When you receive a fin payload you can be sure it's a map.
  - I'd wager the majority of JSON payloads in the wild are objects.
  - In practice there aren't any use cases where forcing a top-level map is infeasible or
    prohibitive.
    - e.g. if the thing you're sending is not a map, maybe a big array or bytes it can easily be
      nested in the top level map. `big_array []`
- Syntax classification: `aggregate`, `expanded`.

### Calls

```
hello["world"]
run(verbose true)
foo[bar[baz[]]
foo[
  bar[
    # this comment expands the bar call
    # which in turn expands the foo call
    # the baz call remains inline
    baz[]
  ]
]

# extension could even be used to rewrite nested calls (multiple tags)
thread_first[[], baz, bar, foo]

# more powerful than a simple list of tags
thread_first[42, inc, add[2, 3]]
add[inc[42], 2, 3]

thread_last[42, inc, add[2, 3]]
add[2, 3, inc[42]]

thread_as[(x 42), foo(a x), add[2, x, 3]]
add[2, foo[a 42], 3]
```

- Unlike array/maps which can contain any number of sub-syntax elements calls always contain exactly
  two sub elements, and they're types are restricted.
  - The first element (the tag) must be a [symbol](#symbols)
  - The second element (the argument) must be an [array](#arrays) or a [map](#maps).
  - The above restrictions allow call syntax to be defined without delimiters.
  - Any symbol that immediately precedes an array or map is a valid call.
  - Calls can be thought of as tagged values.
- While array syntax could be overloaded to provide an extension mechanism (e.g. s-expressions)
  calls provide a simple, obvious extension point for authors.
- Why only a single tag per call?
  - Some formats provide the ability to use multiple tags/annotations? They usually require extra
    syntax delimiters.
  - Extension even in the single-tag form described here is surprisingly powerful.
  - See the `thread_` examples above for how you could build multi-tag semantics using only
    single-tag calls.
- Syntax classification: `aggregate`, `inline` & `expanded`.
  - The call inherits the same form (`inline`/`expanded`) as its argument

## Fin Binary Format

- File extension is `.fib`
- MIME type is `application/fib`?
