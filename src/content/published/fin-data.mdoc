---
kind: "article"
title: "Fin Data"
description: "Describes what Fin data is and how to format it for transmission or storage."
pubDate: "2024-01-01T23:38:33.561Z"
slug: "draft/fin-data"
---

> Alpha. Expect changes.

This document describes what fin data is and how to format it for transmission or storage. This
includes a [data model](#fin-data-model) and two formats.

- A [text format](#fin-text-format) (`.fin` extension) for humans to read, author and manipulate
  with a text editor.
- A [binary format](#fin-binary-format) (`.fib` extension) to more efficiently convey data between
  programs.

Both formats are **self-describing**, meaning aside from the rules outlined in this specification no
additional information or out of band schema is required to produce or interpret it.

## Motivating use cases

- Human authored records (configuration, metadata, documentation, logs etc.)
- Sending values efficiently between programs.
- For use in domains that require precise value types (e.g. financial applications).
- For archival and long term information preservation.
- To describe programs. It should be expressive/capable enough to use as the base syntax for general
  purpose and domain specific programming languages.

{% box %}

**Why another format?** For example, consider YAML which is common place. In fact, I can see some at
the top of this article in my text editor. Here is my favourite review of YAML

> nothing but a labyrinth of white space and inscrutable symbols, a prison for the unwary coder. It
> mocks my intelligence with its simplistic structure and minimal syntax, yet it still manages to
> ensnare me in its deadly embrace. I curse the day I first laid eyes on that ridiculous format, and
> the false promises of ease and clarity it offered me. It has only brought me pain and frustration,
> with its inconsistent indentation and subtle errors lurking in every line.
>
> [Andrew Kelley (creator of the Zig programming language)](https://github.com/ziglang/zig/pull/14265#issue-1528464069)

Also see
[The YAML document from hell](https://ruudvanasseldonk.com/2023/01/11/the-yaml-document-from-hell).

JSON/TOML are certainly better, but also have downsides. Yet these formats are often
[chosen](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-formats.html), even
for DSLs. This [xkcd](https://xkcd.com/927/) comic is a classic, but there's no such thing as too
many ideas, especially if innovation is the goal. I'm frustrated with many existing data formats and
curious if I could do better.

{% /box %}

## Goals

In order of importance.

- **No versioning**. We'd like our formats to be enduring with stable semantics.
  - It's _okay_ to iterate and rapidly prototype prior to release (alpha).
  - If the released spec is broken, deficient or unusable that's also okay. We can go back to the
    drawing board and try again with a different name, a different spec.
  - Adding complexity to data format specs in anticipation of future versions is counterproductive.
    So is handling multiple versions in implementations.
- **Straightforward**. The syntax rules should be simple and obvious. In particular, working with
  fin data should quickly become automatic and take very little mental capacity.
  - In my personal experience JSON has this property. YAML does not.
  - There's nuance to this goal. Less syntax is often better, but care must be taken to **reduce
    overloading** which increases the mental capacity required by us to process syntax.
- **Value semantics**. Fin formats aren't concerned with higher level concepts such as schemas,
  validation or remote procedure calls. There is no concept of objects or references. The formats
  are nothing more than a mechanism to combine and convey values.
- **Interoperability**. We'd like to allow a wide range of systems across diverse domains to convey
  most information seamlessly without resorting to custom application logic (extension).
  - The goal is to choose a **pragmatic set of primitives** that leverage existing standards for
    fundamental data types to establish common ground without introducing unacceptable complexity to
    syntax rules.
  - Certain domains require precise value types. It is preferable to have primitives that support
    this. Capable systems can interchange data seamlessly and less capable systems can decide how to
    act given their constraints.
  - Interoperability is desirable, however the design should not be driven by the choices made in
    existing programming languages or other formats. Encoders/Decoders should take a "best-effort"
    approach when mapping to a given language or non-fin format.
  - As an example, many languages have `null`, `nil`, `undefined` or `void` to represent the
    _absence_ of a value. Fin is about conveying values, so does not encode these types. Leaving
    them out of the data achieves the same result.
- **Extensible**. If the core (first-class) value types support extension, consumers and produces
  can compose them to signal domain-specific semantics and intermediaries can be oblivious to it.
  New specification versions are not required for every new use-case or custom data type.
- **Top Level Streaming**. The formats should support top-level value streams. This means the top
  level value can be conveyed in small chunks over a long-lived connection, and could grow to
  arbitrary length. We consider comprehensive streaming protocols to be a higher level concept (out
  of scope) that could be built on top of this specification.
- **Canonical output**. Text decoders are flexible to the layout of incoming fin data, encoders are
  not. There is one agreed upon way to output fin data and all encoders enforce it. Auto-formatting
  tools (e.g. prettier) are not required as a decode/encode roundtrip produces canonical output.
- **Compactness**. Where possible design choices should be made that lead to compact representation
  and efficient implementations but not at the cost of the goals listed above.
- **Tooling**. Developer experience is very much an auxiliary goal that only makes sense if primary
  goals are met.
  - Reference encoders/decoders and test cases.
  - A Tree-sitter grammar.
  - A language server for text editors.
  - A CLI for inspecting and converting between data formats.
- **Lossless conversion** between fin formats is not a goal.
  - For our benefit the text format provides multiple ways to represent the same value (e.g.
    [raw strings](#raw-strings) vs [escaped strings](#escaped-strings)). [Comments](#comments) are
    also provided for our benefit and do not represent values at all. To support these, text
    encoders and decoders have to keep track of extra metadata.
  - On the other hand the binary format is for efficient value exchange between programs and by
    design does not encode unnecessary metadata. Converting from the fin text format to binary and
    back is likely to produce different output that represents the same value.

## Fin Data Model

The fin data model describes a set of 8 value types.

- 6 primitive types ([byte arrays](#byte-arrays), [strings](#strings), [symbols](#symbols),
  [booleans](#booleans), [numbers](#numbers), [timestamps](#timestamps))
- 2 collection types ([arrays](#arrays), [maps](#maps))
  - combine other values of any type (heterogeneous) including other collections (recursive
    nesting).
  - can optionally be tagged (prefixed) with a symbol to signal domain-specific semantics
    (extensibility).

### Byte Arrays

A byte arrays represents a fixed-length array of bytes (octets).

- Byte arrays are used to directly embed raw binary data.
- No interpretation or processing occurs at all (opaque).
- Empty byte arrays (zero length) are allowed.

### Strings

Strings are used to convey Unicode text.

- The Unicode standard is ubiquitous and makes sense as a core primitive. In almost all cases, if a
  producer wants to convey text, it will be Unicode text. (interoperability)
- Encoders and decoders must ensure strings are a valid sequence of Unicode code points.
- No Unicode canonicalization occurs (out of scope).
- Empty strings are allowed.

### Symbols

Symbols are simplified strings used to represent names and identifiers.

- Restricted to the following code points (`a-z`, `0-9`, `:`, `_`)
  - Subset of the Basic Latin Unicode Block (one-byte in UTF-8).
- Empty (zero length) symbols are invalid.
- Must not start with a digit.
- `true` and `false` are invalid symbols
- The underscore is provided to break up words
  - Only `snake_case` is possible
- The colon is provided to support breaking up the symbol into segments.
  - Enables higher level semantics like namespaces/aliasing.
- Symbols are part of fins extension mechanism (tagged collections).

{% box %}

**Why not use strings?** You could ask a similar question: why have a strings when byte arrays work?
Hopefully it's obvious that strings are more useful to us than opaque blobs of hex or base64. In
practice, the same applies for symbols, they are more useful than strings in certain situations.

One of the [motivating use cases](#motivating-use-cases) for fin data is to describe programs where
local names and identifiers are abundant. Having a restricted and more ergonomic primitive than
strings makes a big difference writing programs (see JSON DSLs). It's not just programming languages
though. If you look at most JSON payloads, object keys are almost always symbols and the use of
strings is unnecessary.

{% /box %}

### Booleans

Booleans have two possible values `true` and `false`.

### Numbers

Numbers are used to represent real numbers.

- Arbitrary precision decimal notation.
- Does not represent special values (`nan`, `inf`, `-inf`) which are not real numbers.
- Includes all integers.
- Can be combined (using tagged collections) to represent exact rationals and complex numbers.

### Timestamps

A timestamp is a date-time record that conveys an exact point in time.

- They have a consistent global definition.
  > A timestamp is a date-time clock measurement along with that clocks **offset** from UTC
  > ([Coordinated Universal Time](https://en.wikipedia.org/wiki/Coordinated_Universal_Timehttps://en.wikipedia.org/wiki/Coordinated_Universal_Time)).
- UTC is the primary time standard globally used to regulate clocks and time.
- A clock measurement includes the current date (conventionally using the Gregorian Calendar)
- Fractional seconds can be used to record time with arbitrary precision.
- It is possible for two different timestamps to represent the same instant in time (clocks with
  different offsets)
- Timestamps do not encode timezone or alternate calendar information. The expectation is authors
  will combine timestamps with other fin primitives for use cases that require it.

{% box %}

**Why not use an extension?** Exact time is a fundamental part of information which is obvious to
anyone keeping historical records. Its use is widespread. By using a timestamp primitive we reduce
the scenario where authors are choosing different ways to represent the same instant in time.
Interoperability improves.

In general, you could make the same argument for many data types (URLs, URIs, UUIDs, money, emails,
regex, etc.) to be blessed as primitives (less overloading). It's a trade-off though, more
primitives mean more syntax rules. It becomes harder for us to parse data at a glance (especially
when taken to the extreme). There's also the awkward scenario when new standards emerge that
_should_ be primitives based on those included in the released spec (extension solves this).

Ultimately we made the judgement call that exact time is fundamental and would benefit the most from
being primitive.

{% /box %}

### Arrays

### Maps

---

**WIP**

## Fin Text Format

- File extension is `.fin`
- MIME type is `application/fin`
- The fin text format is defined in terms of Unicode text (code points).
- Each code point is distinct and no Unicode normalisation is performed.
- The only supported character encoding is UTF-8.
- The UTF-8 byte order mark (BOM) may be stripped if present and disallowed from the remaining
  input.

**TODO** any illegal code points? NUL

### Normalisation

The fin text format is designed to be authored by humans and as such invalid syntax is to be
expected. In cases where the input is strictly invalid, but it can be determined precisely what the
user was intending (unambiguous fix) then decoders should accept the invalid code. They should
provide feedback to the user (via reporting) and fix it. This is a better experience than strictly
failing on invalid input.

**Normalisation only applies when decoding the text format**. This document will explicitly point
out when normalisation should happen. Compliant fin text decoders must provide two modes

- `strict`. Fail immediately when invalid input is found.
- `lax` (default). Collect issues, perform normalisation (as outlined in this document) and continue
  decoding where possible.

#### Global Rules

- Canonical output is free of carriage return (`"\r"`) code points. However, it can almost always
  safely be accepted and removed from canonical output
- Capital letters `A-Z` are invalid except as part of [comment](#comments) and [string](#strings)
  literals and can safely be normalised to lower case.

#### Syntax Forms

- Inline.
  - Starts and ends on the same line.
  - Optionally surrounded by delimiters.
- Block.
  - starts with a delimiter and extends to the end of line. If the first non whitespace code point
    on the next line is the block delimiter then the block expands to include that line, and so on.
- Expanded.
  - Contains sub-syntax that span multiple lines.
  - Optionally surrounded by delimiters.

### Comments

```
# this is a comment which extends to the next two lines.
# comments are verbatim utf-8 sequences with no escaping.
# if required, a space will be inserted after the initial #.

[
  # comments follow normal indentation rules.
  # this comment appears inside an array.
  # as a result the arrays canonical output has expanded.
  1, 2
  3
]

## comments that start with double-hash are auto wrapped at 65
## character width (not including `## `).
```

- Exist only for human communication / documentation. This means they're only found in the text
  format.
- They do not represent values. The array in the above example contains three numbers, the comment
  does not count as a value.
- By definition comments extend at least to the end of the line.
  - It's impossible to output an aggregate map or array on a single line if it contains a comment.
  - This property means authors can use comments to control the canonical text output of aggregates.
- Trailing comments are not allowed and will be moved to a new line.
- Syntax classification: `non-value`, `block`.

### Gaps

**TODO**

- vertical space
- sub-syntax element seen in expanded aggregates.
- Syntax classification: `non-value`, `block`.

### Booleans

```
true
false
```

- Uppercase will be normalised to lowercase.
- Syntax classification: `primitive`, `inline`.

### Symbols

```
abc
_
:println
_abc:d12:e_3
:crypto:sha256
```

- A string-like primitive with no wrapping delimiters.
  - They are a natural fit to represent names and identifiers where a string would be more
    cumbersome.
- `camelCase` and `kebab-case` are accepted but normalised to snake case. Any remaining uppercase
  code points are converted to lowercase.
- Syntax classification: `primitive`, `inline`.

### Strings

String literals are used to convey Unicode text. Text encoders and decoders ensure strings are a
valid sequence of Unicode code points. Two syntax forms are provided for strings.

#### Escaped Strings

```
"this \t string \n supports \\ escape \" sequences \u{1f602}"
"" # empty strings are okay
```

- Newline, backslash and double quote are not allowed unescaped.
- Common escapes. `\n \r \t \" \\`
- Unicode escapes. Code point is specified with hex digits `'\u{1f602}'`.
  - All uppercase is normalised to lowercase.
  - Leading zeros are removed.
- No byte escapes? Are they okay if encoders/decoders enforce valid Unicode?
- Syntax classification: `primitive`, `inline`.

#### Raw Strings

```
|raw string | no escapes so newlines
|and comments are # part of the raw string

[
  # the two strings below are the same
  | another raw string this time indented
  |  #\n|#
  " another raw string this time indented\n  #\\n|#"
]

# below is an empty raw strings
|
```

- Very similar to [comments](#comments) but are values
- also force aggregates to expand
- **TODO** can they be trailing? Does line normalisation still apply?
- Syntax classification: `primitive`, `block`.

**TODO** perhaps single quotes or backticks for inline raw strings?

### Bytes

```
<00>, <de ad be ef> # spaces between bytes/octets?
<> # zero bytes is ok
```

- raw data (byte arrays, bytes) are represented inline in hexadecimal notation between angle
  brackets
- uppercase will be normalised to lowercase.
- Syntax classification: `primitive`, `inline`.

### Numbers

```
0, -1, 42
0.0, -.123, 1.23e+10
```

TODO combine integer?

- Arbitrary precision integers are written verbatim in decimal notation.
- Common prefixes for binary `0b`, octal `0o` and hexadecimal `0x` are accepted but will normalise
  to decimal notation. See [bytes](#bytes) for a hex literal. **TODO** is this the best choice?
- All leading `+` signs will be stripped.
- A single `-` sign is considered part of the integer. It will be stripped from zero.
- Internal underscores `_` are often used to visually separate number digits. They are accepted but
  will be stripped when normalising.
- Syntax classification: `primitive`, `inline`.

- Arbitrary precision decimals
- Similar to integers underscores `_` will be stripped.
- alternate base notation will be converted to decimal.
- Syntax classification: `primitive`, `inline`.

### Timestamps

```
2024-01-22T22:53:52.548Z
2007-02-23T12:14:33.079-08:00
```

- strict form of RFC3339. normalisations?
- Syntax classification: `primitive`, `inline`.

### Arrays

```

# inline
[1, 2, true]

[
  # expanded
  42
]

```

- Syntax classification: `aggregate`, `inline` & `expanded`.

### Maps

```
# call to array of key-value pairs. 1 symbol, 6 delimiters, 3 commas
map[[k1, v1], [k2, v2]]

# same but using map syntax. 2 delimiters, 1 comma
(k1 v1, k2 v2)
```

- Technically an aggregate for a sequence of items (e.g. [arrays](#arrays)) is enough for a working
  data format. For example consider lisp and s-expressions.
- Why have different syntax for maps?
  - Maps are everywhere. Most JSON payloads have more objects than arrays.
  - If built on top of arrays. They require significantly more delimiters and commas.
  - A single starting symbol is required to signal to the reader that it's a map which may not
    always be visible.
  - A different syntax is less verbose and makes it immediately obvious it's a map.
- Other than the syntax change semantically it's equiv to an array of key value pairs
  - Two maps with different order are two different values. Is this ok?
  - Consumers can decide if order is significant or not
- Syntax classification: `aggregate`, `inline` & `expanded`.

#### Top-level Map

```
a 1, b 2
c 3

letters ["a", "b", "c"]
numbers [1, 2, 3]
```

- The top level value is always an implicit, expanded map. Implicit means the opening/closing map
  delimiters `()` are implied when processing the top level (root) value.
- Key value pairs are listed with no indentation.
- Otherwise all the same syntax rules of an expanded map apply.
- When you receive a fin payload you can be sure it's a map.
  - I'd wager the majority of JSON payloads in the wild are objects.
  - In practice there aren't any use cases where forcing a top-level map is infeasible or
    prohibitive.
    - e.g. if the thing you're sending is not a map, maybe a big array or bytes it can easily be
      nested in the top level map. `big_array []`
- Syntax classification: `aggregate`, `expanded`.

### Calls

**TODO** remove

```
hello["world"]
run(verbose true)
foo[bar[baz[]]
foo[
  bar[
    # this comment expands the bar call
    # which in turn expands the foo call
    # the baz call remains inline
    baz[]
  ]
]

# extension could even be used to rewrite nested calls (multiple tags)
thread_first[[], baz, bar, foo]

# more powerful than a simple list of tags
thread_first[42, inc, add[2, 3]]
add[inc[42], 2, 3]

thread_last[42, inc, add[2, 3]]
add[2, 3, inc[42]]

thread_as[(x 42), foo(a x), add[2, x, 3]]
add[2, foo[a 42], 3]
```

- Unlike array/maps which can contain any number of sub-syntax elements calls always contain exactly
  two sub elements, and they're types are restricted.
  - The first element (the tag) must be a [symbol](#symbols)
  - The second element (the argument) must be an [array](#arrays) or a [map](#maps).
  - The above restrictions allow call syntax to be defined without delimiters.
  - Any symbol that immediately precedes an array or map is a valid call.
  - Calls can be thought of as tagged values.
- While array syntax could be overloaded to provide an extension mechanism (e.g. s-expressions)
  calls provide a simple, obvious extension point for authors.
- Why only a single tag per call?
  - Some formats provide the ability to use multiple tags/annotations? They usually require extra
    syntax delimiters.
  - Extension even in the single-tag form described here is surprisingly powerful.
  - See the `thread_` examples above for how you could build multi-tag semantics using only
    single-tag calls.
- Syntax classification: `aggregate`, `inline` & `expanded`.
  - The call inherits the same form (`inline`/`expanded`) as its argument

## Fin Binary Format

- File extension is `.fib`
- MIME type is `application/fib`?
